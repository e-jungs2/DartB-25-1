# 캐글로리 4주차
## House Prices - Advanced Regression Techniques
### EDA
```sql
quantitative = [f for f in train.columns if train.dtypes[f] != 'object']
quantitative.remove('SalePrice')
quantitative.remove('Id')
qualitative = [f for f in train.columns if train.dtypes[f] == 'object']
```
이렇게 범주형 변수와 수치형 변수 나눠주는 거 good!
모델링 전처리 할 때 좋겠다. 인코딩이나, 스케일링..

```sql
test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01
normal = pd.DataFrame(train[quantitative])
normal = normal.apply(test_normality)
print(not normal.any())
```
모든 수치형 변수들이 정규성을 만족하는지 테스트 하는 코드.
정규성 만족안하면 로그변환해주거나, 비모수 검정하거나.. 좋을듯.

```sql
def spearman(frame, features):
    spr = pd.DataFrame()
    spr['feature'] = features
    spr['spearman'] = [frame[f].corr(frame['SalePrice'], 'spearman') for f in features]
    spr = spr.sort_values('spearman')
    plt.figure(figsize=(6, 0.25*len(features)))
    sns.barplot(data=spr, y='feature', x='spearman', orient='h')

features = quantitative + qual_encoded
#spearman(train, features)

plt.figure(1)
corr = train[quantitative+['SalePrice']].corr()
sns.heatmap(corr)
plt.figure(2)
corr = train[qual_encoded+['SalePrice']].corr()
sns.heatmap(corr)
plt.figure(3)
corr = pd.DataFrame(np.zeros([len(quantitative)+1, len(qual_encoded)+1]), index=quantitative+['SalePrice'], columns=qual_encoded+['SalePrice'])
for q1 in quantitative+['SalePrice']:
    for q2 in qual_encoded+['SalePrice']:
        corr.loc[q1, q2] = train[q1].corr(train[q2])
sns.heatmap(corr)
```
피어슨 상관계수와 스피어만 상관계수가 다름..
| 항목     | Pearson         | Spearman              |
| ------ | --------------- | --------------------- |
| 측정 대상  | 선형 관계           | 순위 기반 단조 관계           |
| 데이터 조건 | 정규분포, 연속형       | 정규성 불필요, 순서형 가능       |
| 이상치 영향 | 큼               | 작음                    |
| 사용 예   | 연속형 변수 간 선형성 확인 | 비선형 또는 순서형 변수 간 관계 분석 |

```sql
features = quantitative + qual_encoded
model = TSNE(n_components=2, random_state=0, perplexity=50)
X = train[features].fillna(0.).values
tsne = model.fit_transform(X)

std = StandardScaler()
s = std.fit_transform(X)
pca = PCA(n_components=30)
pca.fit(s)
pc = pca.transform(s)
kmeans = KMeans(n_clusters=5)
kmeans.fit(pc)

fr = pd.DataFrame({'tsne1': tsne[:,0], 'tsne2': tsne[:, 1], 'cluster': kmeans.labels_})
sns.lmplot(data=fr, x='tsne1', y='tsne2', hue='cluster', fit_reg=False)
print(np.sum(pca.explained_variance_ratio_))
```
고차원 데이터를 2차원으로 비선형 축소.
PCA로 차원 축소
KMeans가 군집화 수행

```sql
train.drop(['Id'], axis=1, inplace=True)
test.drop(['Id'], axis=1, inplace=True)

train = train[train.GrLivArea < 4500]
train.reset_index(drop=True, inplace=True)
train["SalePrice"] = np.log1p(train["SalePrice"])
y = train['SalePrice'].reset_index(drop=True)

### Features

train_features = train.drop(['SalePrice'], axis=1)
test_features = test
features = pd.concat([train_features, test_features]).reset_index(drop=True)

features.shape

features['MSSubClass'] = features['MSSubClass'].apply(str)
features['YrSold'] = features['YrSold'].astype(str)
features['MoSold'] = features['MoSold'].astype(str)
features['Functional'] = features['Functional'].fillna('Typ')
features['Electrical'] = features['Electrical'].fillna("SBrkr")
features['KitchenQual'] = features['KitchenQual'].fillna("TA")
features["PoolQC"] = features["PoolQC"].fillna("None")
features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])
features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])
features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])


features.head()

for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    features[col] = features[col].fillna(0)

for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:
    features[col] = features[col].fillna('None')

for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    features[col] = features[col].fillna('None')

features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))

features.head()

objects = []
for i in features.columns:
    if features[i].dtype == object:
        objects.append(i)
features.update(features[objects].fillna('None'))

features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))

numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerics = []
for i in features.columns:
    if features[i].dtype in numeric_dtypes:
        numerics.append(i)
features.update(features[numerics].fillna(0))

# 왜도 계산 (quantitative 변수만)
skew_features = train[quantitative].apply(lambda x: skew(x.dropna()))
high_skew = skew_features[skew_features > 0.5]
skew_index = high_skew.index

# Box-Cox 변환
for col in skew_index:
    try:
        clipped = train[col].clip(lower=1e-6)
        if (clipped + 1).min() > 0:
            lmbda = boxcox_normmax(clipped + 1)
            train[col] = boxcox1p(clipped, lmbda)
        else:
            print(f"Skipping '{col}' due to non-positive values.")
    except Exception as e:
        print(f"Skipping '{col}' due to error: {e}")

features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)

features['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']
features['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']

features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +
                                 features['1stFlrSF'] + features['2ndFlrSF'])

features['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +
                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))

features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +
                              features['EnclosedPorch'] + features['ScreenPorch'] +
                              features['WoodDeckSF'])


features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)
features['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)
features['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
features['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)
features['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0) 
```

1. 기본 정제 및 타겟 변수 전처리
Id 컬럼 제거 (모델에 불필요한 식별자)

극단적으로 큰 면적(GrLivArea >= 4500) 제거 → 이상치 제거

타겟 변수(SalePrice)에 log1p() 적용 → 정규성 확보 및 스케일 안정화

2. 훈련/테스트 통합 처리
SalePrice 제외한 train_features와 test 데이터를 수직 결합

이후 모든 전처리 작업은 features에서 일괄 수행

3. 범주형 변수 처리
MSSubClass, YrSold, MoSold → 문자열로 변환 (범주형으로 취급하기 위함)
- 결측값 채우기:
Functional, Electrical, KitchenQual, PoolQC, Exterior1st/2nd, SaleType 등은 최빈값 또는 특정값으로 대체.

Garage, Bsmt 관련 변수들: 존재하지 않음을 의미하는 'None'으로 처리
MSZoning: MSSubClass 그룹별 mode로 채움.

나머지 object 타입 범주형 변수들: 'None'으로 일괄 대체.

4. 수치형 변수 결측값 처리
LotFrontage: Neighborhood 기준으로 중앙값 채움

나머지 수치형 변수들은 모두 0으로 채움

5. 왜도(Skewness) 보정
quantitative 변수 중 왜도 > 0.5인 변수들만 선별

Box-Cox 변환으로 비정규 분포를 정규에 가깝게 조정 (log1p와 유사하지만 자동 람다 선택)

6. 불필요한 변수 제거
정보량이 적거나 불균형한 변수(Utilities, Street, PoolQC) 제거

7. 파생변수 생성 (Feature Engineering)
YrBltAndRemod: 건축 연도 + 리모델링 연도

TotalSF: 지하 + 1층 + 2층 면적 합

Total_sqr_footage: 지하 거주 공간 + 1층 + 2층

Total_Bathrooms: 전체 욕실 수 (반욕실 0.5 처리 포함)

Total_porch_sf: 전체 현관/베란다/데크 면적 합

haspool, has2ndfloor, hasgarage, hasbsmt, hasfireplace: 특정 조건 존재 여부를 나타내는 이진 변수들.

```sql
def blend_models_predict(X):
    return ((0.1 * elastic_model_full_data.predict(X)) + \
            (0.05 * lasso_model_full_data.predict(X)) + \
            (0.1 * ridge_model_full_data.predict(X)) + \
            (0.1 * svr_model_full_data.predict(X)) + \
            (0.1 * gbr_model_full_data.predict(X)) + \
            (0.15 * xgb_model_full_data.predict(X)) + \
            (0.1 * lgb_model_full_data.predict(X)) + \
            (0.3 * stack_gen_model.predict(np.array(X)))) 
```
예측 성능 향상을 위해 여러 모델의 예측값을 혼합(blend) 하는 함수.-> 블렌딩
| 모델                        | 비중  | 설명                          |
| ------------------------- | --- | --------------------------- |
| `elastic_model_full_data` | 10% | ElasticNet 회귀               |
| `lasso_model_full_data`   | 5%  | Lasso 회귀 (L1)               |
| `ridge_model_full_data`   | 10% | Ridge 회귀 (L2)               |
| `svr_model_full_data`     | 10% | 서포트 벡터 회귀                   |
| `gbr_model_full_data`     | 10% | Gradient Boosting Regressor |
| `xgb_model_full_data`     | 15% | XGBoost                     |
| `lgb_model_full_data`     | 10% | LightGBM                    |
| `stack_gen_model`         | 30% | Stacking을 통해 학습한 메타 모델      |

| 모델명                       | 클래스 이름 (추정)                                           | 설명                                                                                                          |
| ------------------------- | ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| `elastic_model_full_data` | `ElasticNet` (from `sklearn.linear_model`)            | L1 + L2 페널티를 결합한 모델. 과적합 방지 효과 있음.                                                                          |
| `lasso_model_full_data`   | `Lasso` (from `sklearn.linear_model`)                 | L1 정규화를 사용하는 회귀 모델. 계수 일부를 0으로 만들어 **변수 선택 효과**가 있음.                                                        |
| `ridge_model_full_data`   | `Ridge` (from `sklearn.linear_model`)                 | L2 정규화를 사용하는 모델. 계수를 축소시켜 다중공선성 문제를 완화함.                                                                    |
| `svr_model_full_data`     | `SVR` (from `sklearn.svm`)                            | 서포트 벡터 회귀. 비선형 회귀에 적합하며, **커널 트릭**을 사용함. 느리지만 강력함.                                                          |
| `gbr_model_full_data`     | `GradientBoostingRegressor` (from `sklearn.ensemble`) | 순차적으로 약한 모델을 결합하는 방식의 **부스팅 회귀기**. 예측 성능이 높고 튜닝 여지가 많음.                                                     |
| `xgb_model_full_data`     | `XGBRegressor` (from `xgboost`)                       | **GradientBoosting**을 최적화한 버전. 빠르고 정교한 모델이며 캐글에서 매우 인기 많음.                                                  |
| `lgb_model_full_data`     | `LGBMRegressor` (from `lightgbm`)                     | XGBoost보다 더 빠르고 메모리 효율적인 **경량 부스팅 모델**. 대용량 데이터에 적합.                                                        |
| `stack_gen_model`         | 보통 `StackingCVRegressor` 또는 커스텀 스태커                   | 위 모델들의 예측값을 입력으로 받아 학습하는 **메타 모델**. 일반적으로 `LinearRegression`, `Ridge`, `Lasso`, `XGB` 등을 사용해서 앙상블 성능을 극대화함. |

.
.
.
Lasso, Ridge, ElasticNet: 선형 계열, 각기 다른 정규화 방식 사용

SVR: 비선형 복잡도 있는 회귀

GBR, XGB, LGB: 부스팅 기반의 강력한 비선형 모델

Stacking 모델: 위 모델들을 기반으로 종합 판단하는 최종 결정자
.
.
.
블렌딩 + 스테킹.
| 모델                                       | 역할                                              |
| ---------------------------------------- | ----------------------------------------------- |
| **Lasso, Ridge, ElasticNet**             | 숫자 계산에 강한 통계학자들. 수치를 기반으로 차분하게 예측함.             |
| **SVR**                                  | 패턴을 잘 포착하는 분석가. 복잡한 곡선 같은 것도 잘 잡아냄.             |
| **Gradient Boosting, XGBoost, LightGBM** | 실무에 강한 데이터 사이언티스트. 여러 가지 판단 기준을 조금씩 조합해서 결정함.   |
| **Stacking Model**                       | 위의 전문가들이 낸 예측값을 보고 “종합 판단”을 내리는 **심사위원** 같은 역할. |

집값데이터는 복잡하고 비선형적,
서로 다른 모델은 서로 다른 강점을 지님.

3. 앙상블은 성능을 높이고 위험을 줄이는 전략.

<앙상블의 흐름>
1. 데이터 전처리 및 피처 엔지니어링
2. 여러 개의 개별 모델 학습 (Lasso, Ridge, XGBoost 등)
3. 각 모델의 예측값을 수집
4. (스태킹) 예측값을 기반으로 meta-model 학습 (예: Ridge, XGB 등)
5. (블렌딩) 여러 모델 + 스태킹 모델의 예측을 가중 평균
6. 최종 예측 결과 생성